# -*- coding: utf-8 -*-
"""Banking_Analytics_Credit_Card_Scoring.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1swcWZmxOKUCf7hUnZYe8kLf7WcJ8KnqO

***Credit card lending is one of the most common offerings in modern Fintechs. Usually granted by a 
bank, these products are now being granted by a Fintech that acts as a front for a bank that actually 
takes the risk. Deciding who to grant these services is an interesting problem under these 
circumstances.
In this coursework, you will develop a fully compliant PD model from the data they make available, 
from the raw data to the level 2 calibration, using what you have learned in the lectures. The 
objective of the coursework is to estimate the capital requirements for the credit card company as 
if they were a bank. 
You are given information from approximately 50,000 credit cards. The data includes information 
from the application to the credit card in Brazil during 2007, some of which can be used to predict 
the performance of the loan. The variable description is available in the Excel file 
“CC_VariablesList.xls”***
"""

from google.colab import drive
drive.mount("/content/drive", force_remount=True)

!pip install scorecardpy

!ls drive//MyDrive/CC_Modeling_Data.txt

!ls drive/MyDrive/
import pandas as pd

filename = 'drive//MyDrive/CC_Modeling_Data.txt'

import pandas as pd
import chardet
with open(filename, 'rb') as f:
    result = chardet.detect(f.read())

result

df = pd.read_csv(filename, header=None, sep='\t', engine='python', encoding=result['encoding'])
df.head(2)

filename1 = 'drive//MyDrive/CC_VariablesList.XLS'
df1 = pd.read_excel(filename1, 0)
df1.head()

df.columns = df1['Var_Title']
df.rename({'TARGET_LABEL_BAD=1':'default'}, axis=1, inplace=True)

import numpy as np
df.select_dtypes(exclude=np.number).columns

df.head()

mylist = []

for i in df.columns:
  if len(set(df[i])) ==1:
    print("The columns %s has only one value" %i)
    print(type(df.iloc[6][i]))
    a = set(df[i])
    print(a)
    if len(a)==1:
      mylist.append(i)

print(mylist)

from matplotlib import pyplot as plt
mylist.remove('EDUCATION_LEVEL')
mylist.remove('EDUCATION_LEVEL')

fig, axes = plt.subplots(len(mylist),1)

for ii, i in enumerate(mylist):
  axes[ii].hist(df[i])
plt.show()

"""**Removal of variables with single value**"""

list_to_remove = ['CLERK_TYPE', 'QUANT_ADDITIONAL_CARDS', 'FLAG_MOBILE_PHONE', 'FLAG_HOME_ADDRESS_DOCUMENT', 'FLAG_RG', 'FLAG_CPF', 'FLAG_INCOME_PROOF', 'FLAG_ACSP_RECORD']

for i in list_to_remove:
  if i in df.columns:
    df.drop([i], axis=1, inplace=True)

df.describe()

"""**Education Level is a duplicate variable**"""

import numpy as np

A = np.zeros(len(df['EDUCATION_LEVEL'])) 
B = np.zeros(len(df['EDUCATION_LEVEL'])) 
C = np.zeros(len(df['EDUCATION_LEVEL'])) 

for ii, i in enumerate(range(0, 50000)):
  
  new_list = []
  my_list = []
  my_list = df.iloc[ii]['EDUCATION_LEVEL']

  B[ii] = my_list[0]
  C[ii] = my_list[1]

  new_list = [item for item in my_list if not(np.isnan(item)) == True]
  if (len(new_list)>0):
    A[ii] = np.max(new_list)
  else:
    A[ii] = np.nan

A[A==0] = np.nan
df['education'] = A
df['education1'] = B
df['education2'] = C

df.describe()

df.drop(['EDUCATION_LEVEL'], axis=1, inplace=True)
df.drop(['education'], axis=1, inplace=True)
df.drop(['education1'], axis=1, inplace=True)

"""**Here I removed variables related to the identity of an applicant or variables which do not contribute to the creditworthiness of an applicant**"""

cols_to_remove = ['ID_CLIENT', 'FLAG_EMAIL', 'APPLICATION_SUBMISSION_TYPE', 'SEX', 'STATE_OF_BIRTH', 'CITY_OF_BIRTH','NACIONALITY']
for i in cols_to_remove:
  if i in df.columns:
    df.drop([i], axis=1, inplace=True)

null_columns = df.columns[df.isnull().any()]
df[null_columns].isnull().sum()

"""Most of 'PROFESSIONAL_CITY', 'PROFESSIONAL_BOROUGH' variables are NoNs. Here, I create a dummy variable 1 is the applicant has PROFESSIONAL_CITY otherwise 0. THen I removed 'PROFESSIONAL_CITY', 'PROFESSIONAL_BOROUGH' since they did not help with predicting the Target Variable"""

df['dummy_profession'] = df['PROFESSIONAL_CITY']
df.loc[df['dummy_profession'].isnull()==False, 'dummy_profession'] = 1
df.loc[df['dummy_profession'].isnull(), 'dummy_profession'] = 0

# mylist = ['PROFESSIONAL_CITY', 'PROFESSIONAL_BOROUGH', 'MATE_PROFESSION_CODE']
# for i in mylist:
#   if i in df.columns:
#     df.drop([i], axis=1, inplace=True)

"""Identifying No-Data Values"""

df['MARITAL_STATUS'] = df['MARITAL_STATUS'].replace({0: np.nan})

# my_list1 = ['RESIDENCE_TYPE', 'PROFESSION_CODE', 'OCCUPATION_TYPE', 'PROFESSIONAL_CITY', 'PROFESSIONAL_BOROUGH', 'MATE_PROFESSION_CODE']

my_list2 = ['MONTHS_IN_RESIDENCE']

# for i in my_list1:
#   print("For the columns %s " %(i))
#   print("The number of nans is %f" %len((df.loc[df[i].isnull(), i])))

#   # df.loc[df[i]==0, i] = np.nan
#   df.loc[df[i].isnull(), i] = np.nan

for i in my_list2:
  print("For the columns %s " %(i))
  print("The number of nans is %f" %len((df.loc[df[i].isnull(), i])))

  df.loc[df[i].isnull(), i] = 0

df.describe()

# df.select_dtypes(exclude=np.number).isnull().any()

null_columns = df.columns[df.isnull().any()]
df[null_columns].isnull().sum()

for i in df.columns:

  b = df.loc[df[i] == 0, i]
  if len(b)>0:
    print(" Columns %s ----%f" %(i, len(b)))

# RESIDENCIAL_PHONE_AREA_CODE, PROFESSIONAL_STATE, PROFESSIONAL_PHONE_AREA_CODE
# df.loc[(df['PROFESSIONAL_CITY'].isnull()) & (df['COMPANY']== 'N')]
# df.loc[df['PROFESSIONAL_CITY'].isnull()]
# plt.hist(A['PERSONAL_MONTHLY_INCOME'], 50)
# plt.show()

df['RESIDENCIAL_PHONE_AREA_CODE_new'] = df['RESIDENCIAL_PHONE_AREA_CODE']
df.loc[(df['FLAG_RESIDENCIAL_PHONE']=='N') & (df['RESIDENCIAL_PHONE_AREA_CODE_new']==' '),'RESIDENCIAL_PHONE_AREA_CODE_new'] = -1
df.loc[(df['FLAG_RESIDENCIAL_PHONE']=='Y') & (df['RESIDENCIAL_PHONE_AREA_CODE_new']==' '),'RESIDENCIAL_PHONE_AREA_CODE_new'] = np.nan

df['PROFESSIONAL_PHONE_AREA_CODE_new'] = df['PROFESSIONAL_PHONE_AREA_CODE']
df.loc[(df['FLAG_PROFESSIONAL_PHONE']=='N') & (df['PROFESSIONAL_PHONE_AREA_CODE_new']==' '),'PROFESSIONAL_PHONE_AREA_CODE_new'] = -1
df.loc[(df['FLAG_PROFESSIONAL_PHONE']=='Y') & (df['PROFESSIONAL_PHONE_AREA_CODE_new']==' '),'PROFESSIONAL_PHONE_AREA_CODE_new'] = np.nan

# df.loc[(df['PROFESSIONAL_CITY'].isnull()) & (df['PROFESSIONAL_ZIP_3']== '#DIV/0!')]
df.loc[df['PROFESSIONAL_ZIP_3']== '#DIV/0!', 'PROFESSIONAL_ZIP_3'] = np.nan
df.loc[df['RESIDENCIAL_ZIP_3']== '#DIV/0!', 'RESIDENCIAL_ZIP_3'] = np.nan

df.loc[df['PROFESSIONAL_STATE']== ' ', 'PROFESSIONAL_STATE'] = np.nan

df.loc[df['RESIDENCIAL_BOROUGH']== ' ', 'RESIDENCIAL_BOROUGH'] = np.nan

mylist = ['PROFESSIONAL_PHONE_AREA_CODE', 'RESIDENCIAL_PHONE_AREA_CODE']
for i in mylist:
  df.drop([i], axis=1, inplace=True)

print(len(df.loc[df['PROFESSIONAL_PHONE_AREA_CODE_new']== ' ']))

Threshold = 0.5
mylist = []
for ii in df.columns:
  t = df[ii].isnull().sum()/len(df)
  if t> Threshold:
    mylist.append(ii)
    print(' The columns is %s %f' %(ii,t))

print(mylist)
for i in mylist:
  df.drop([i], axis=1, inplace=True)

"""**Remove rows if more than 70% of data is null - No records were found**


"""

threshold = 0.7
for ii in range(0, len(df)):

  t = df.iloc[ii].isnull().sum()
  if t > len(df.columns) * threshold:
    print(ii)

df.columns

"""**Outlier removal ** ****"""

# Commented out IPython magic to ensure Python compatibility.
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
# %matplotlib inline

sns.set(color_codes=True)
df3 = df.select_dtypes(include=np.number)
print(len(df3.columns))
for col_id in df3.columns:
  sns.displot(data = df3, x = df3[col_id], hue = "default", kind = 'kde')

C = df['FLAG_VISA'].values + df['FLAG_MASTERCARD'].values + df['FLAG_DINERS'].values + df['FLAG_AMERICAN_EXPRESS'].values + df['FLAG_OTHER_CARDS'].values

df['Total_cards'] = C

D = (df['PERSONAL_MONTHLY_INCOME'].values + df['OTHER_INCOMES'].values)/(1+ df['QUANT_DEPENDANTS'].values)

E = df['OTHER_INCOMES'].values + df['QUANT_DEPENDANTS'].values

df['Income_per_person'] = D

df['Total_Income'] = E

fig, (ax1, ax2, ax3) = plt.subplots(1, 3)

ax1.hist(df['Total_cards'])
ax1.set_title('Total_cards')
ax2.hist(df['Income_per_person'])
ax2.set_title('Income_per_person')
ax3.hist(df['Total_Income'])
ax3.set_title('Total_Income')

# # fig = sns.displot(df['Total_cards'], kind = 'hist')
# print(np.max(C))
plt.show()

from matplotlib import pyplot as plt

fig, axes = plt.subplots(1, 2)
axes[0].hist(df['PERSONAL_MONTHLY_INCOME'])
df = df[df['PERSONAL_MONTHLY_INCOME']<4000]
axes[1].hist(df['PERSONAL_MONTHLY_INCOME'])
plt.title('PERSONAL_MONTHLY_INCOME')
plt.show()

fig, axes = plt.subplots(1, 2)
axes[0].hist(df['MONTHS_IN_RESIDENCE'])
df = df[df['MONTHS_IN_RESIDENCE']< 50]
axes[1].hist(df['MONTHS_IN_RESIDENCE'])
plt.title('MONTHS_IN_RESIDENCE')

plt.show()

"""**Only 71 people with more than 0 months in the job, should remove this variable**"""

# fig, axes = plt.subplots(1, 2)
# axes[0].hist(df['MONTHS_IN_THE_JOB'])
# # df = df[df['MONTHS_IN_THE_JOB']< 50]
# axes[1].hist(df.loc[df['MONTHS_IN_THE_JOB']>0, 'MONTHS_IN_THE_JOB'])
# plt.title('MONTHS_IN_THE_JOB')

# print(len(df.loc[df['MONTHS_IN_THE_JOB']>0, 'MONTHS_IN_THE_JOB']))
# plt.show()

"""**The age below 18 is removed from the data**"""

# print(df.loc[df['AGE']< 18, 'AGE'])
fig, axes = plt.subplots(1, 2)
axes[0].hist(df['AGE'])
df.loc[df['AGE']< 18, 'AGE'] = np.nan
df.loc[df['AGE']> 80, 'AGE'] = np.nan
axes[1].hist(df['AGE'])
plt.title('AGE')

plt.show()

df.columns

df['OTHER_INCOMES_dummy'] =  df['OTHER_INCOMES']
df.loc[df['OTHER_INCOMES_dummy']>0, 'OTHER_INCOMES_dummy'] = 1

df['PERSONAL_ASSETS_VALUE_dummy'] =  df['PERSONAL_ASSETS_VALUE']
df.loc[df['PERSONAL_ASSETS_VALUE_dummy']>0, 'PERSONAL_ASSETS_VALUE_dummy'] = 1

# fig, axes = plt.subplots(1, 2)
# axes[0].hist(df['OTHER_INCOMES'])
# df = df[df['OTHER_INCOMES']<2000]
# axes[1].hist(df.loc[(df['OTHER_INCOMES']<2000) & (df['OTHER_INCOMES']>0), 'OTHER_INCOMES'])
# # axes[1].hist(df.loc[df['OTHER_INCOMES']<1000, 'OTHER_INCOMES'])

# plt.title('OTHER_INCOMES')

# plt.show()

from matplotlib import pyplot as plt

fig, axes = plt.subplots(1, 2)
axes[0].hist(df['QUANT_DEPENDANTS'])
df.loc[df['QUANT_DEPENDANTS']>15, 'QUANT_DEPENDANTS'] = np.nan
axes[1].hist(df['QUANT_DEPENDANTS'])
plt.title('QUANT_DEPENDANTS')

plt.show()

# fig, axes = plt.subplots(1, 2)
# axes[0].hist(df['PERSONAL_ASSETS_VALUE'])
# df = df[df['PERSONAL_ASSETS_VALUE'] < 100000]
# axes[1].hist(df.loc[(df['PERSONAL_ASSETS_VALUE'] < 120000) & (df['PERSONAL_ASSETS_VALUE'] > 0)]['PERSONAL_ASSETS_VALUE'])
# # axes[1].hist(df['PERSONAL_ASSETS_VALUE'])
# plt.title('PERSONAL_ASSETS_VALUE')

# plt.show()

df.columns

# RESIDENCIAL_PHONE_AREA_CODE, PROFESSIONAL_STATE, PROFESSIONAL_PHONE_AREA_CODE
print(len(df['RESIDENCIAL_BOROUGH'].unique()))

# df.loc[df['OCCUPATION_TYPE']==0, 'OCCUPATION_TYPE'] = 3

len(df.loc[df['OCCUPATION_TYPE']==3])

"""***Replace Null Values***"""

df.isnull().any()

mylist = ['MARITAL_STATUS', 'OCCUPATION_TYPE', 'PROFESSION_CODE', 'RESIDENCE_TYPE', 'education2', 'RESIDENCIAL_ZIP_3', 'PROFESSIONAL_ZIP_3', 'RESIDENCIAL_PHONE_AREA_CODE_new', 
          'PROFESSIONAL_PHONE_AREA_CODE_new', 'RESIDENCIAL_BOROUGH']


for i in mylist:
  if i in df.columns:
    repl = df.loc[:,i].mode()
    print(repl[0])
    df[i].fillna(value = repl[0], inplace=True)


mylist = ['MONTHS_IN_RESIDENCE', 'AGE', 'QUANT_DEPENDANTS']

for i in mylist:
  if i in df.columns:  
    repl = df.loc[:,i].median(skipna=True)
    print(repl)
    if i == 'QUANT_DEPENDANTS':
      df[i].fillna(value = np.floor(repl), inplace=True)
    else:
      df[i].fillna(value = repl, inplace=True)

df.columns

j = 'PROFESSIONAL_ZIP_3'
li = df[j].unique()

print(type(df.iloc[2][j]))

# df.select_dtypes(exclude=np.number).isnull().any()
df.isnull().any()



# Commented out IPython magic to ensure Python compatibility.
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
# %matplotlib inline

df3 = df.select_dtypes(include=np.number)
df3.head()

# df['RESIDENCIAL_ZIP_3_numeric'] = 
df.dtypes

df.columns

df3 = df.select_dtypes(exclude=np.number)
print(df3.columns)
# li = ['RESIDENCIAL_STATE',
#        'FLAG_RESIDENCIAL_PHONE',
#        'RESIDENCIAL_ZIP_3',
#        'PROFESSIONAL_ZIP_3']
# li = ['RESIDENCIAL_ZIP_3']
# print(len(df['PROFESSIONAL_PHONE_AREA_CODE'].unique()))
# print(len(df3.loc[df['RESIDENCIAL_CITY']=='ARACATUBA', 'RESIDENCIAL_CITY']))
# for i in li:
#   print(len(df3[i].unique()))
#   for j in df3[i].unique():
#     if len(df3.loc[df[i]==j])<5:
#       print("column name %s, class name %s" %(i, j))

# df.to_csv('drive//MyDrive/df_assignment2.csv')

import pandas as pd
df = pd.read_csv('drive//MyDrive/df_assignment2.csv')
list_to_remove = ['RESIDENCIAL_CITY', 'RESIDENCIAL_BOROUGH', 'PROFESSIONAL_BOROUGH', 'PROFESSIONAL_CITY']

for i in list_to_remove:
  if i in df.columns:
    df.drop([i], axis=1, inplace=True)

list_to_remove = ['FLAG_AMERICAN_EXPRESS', 'FLAG_DINERS', 'POSTAL_ADDRESS_TYPE', 'PERSONAL_ASSETS_VALUE_dummy', 'MONTHS_IN_THE_JOB', 'FLAG_OTHER_CARDS', 'PERSONAL_ASSETS_VALUE']

for i in list_to_remove:
  if i in df.columns:
    df.drop([i], axis=1, inplace=True)

df.columns

import scorecardpy as sc

train, test = sc.split_df(df,
                          y = 'default',
                          ratio = 0.7,
                          seed = 250542531).values()

train.iloc[:, ~train.columns.isin(['default'])]

import numpy as np

bins = sc.woebin(train, y= 'default', 
                 min_perc_fine_bin=0.01, # How many bins to cut initially into
                 min_perc_coarse_bin=0.05,  # Minimum percentage per final bin
                 stop_limit=0.005, # Minimum information value 
                 max_num_bin=6, # Maximum number of bins
                 method='tree'
                 )

sc.woebin_plot(bins)

breaks_adj = sc.woebin_adj(train, "default", bins, adj_all_var = True)

breaks_adj = {'AGE': [23.0,33.0,47.0,53.0], 
     'MARITAL_STATUS': [2.0,3.0,5.0], 
     'RESIDENCIAL_PHONE_AREA_CODE_new': [0.0,41.0,63.0,84.0,91.0],
     'PAYMENT_DAY': [15.0,20.0,25.0 ], 
     'OCCUPATION_TYPE': [1.0,2.0,4.0,5.0 ],
     'FLAG_RESIDENCIAL_PHONE': ['Y','N'],
     'PROFESSIONAL_ZIP_3': [400.0,620.0,660.0,840.0,950.0],     
     'RESIDENCIAL_STATE': ['SC%,%RO%,%RS','PR%,%MA%,%PB','AP%,%MG%,%SP%,%MS%,%PA%,%MT%,%PI','GO%,%RJ%,%CE','RR%,%PE%,%ES%,%RN%,%BA%,%AC%,%TO','AL%,%AM%,%DF%,%SE'], 
     'RESIDENCIAL_ZIP_3': [400.0,620.0,660.0,840.0,950.0]}
#  bins_adj = sc.woebin(train, y="default", breaks_list=breaks_adj) # Apply new cuts

bins_adj = sc.woebin(train, y="default", breaks_list=breaks_adj) # Apply new cuts
train_woe = sc.woebin_ply(train, bins_adj) # Calculate WoE dataset (train)
test_woe = sc.woebin_ply(test, bins_adj) # Calculate WoE dataset (test)
df_woe = sc.woebin_ply(df, bins_adj)

# breaks_adj = '{'AGE': [23.0,33.0,47.0,53.0], 'COMPANY': ['N','Y'], 'FLAG_MASTERCARD': [1.0], 'FLAG_PROFESSIONAL_PHONE': ['N','Y'], 'FLAG_RESIDENCIAL_PHONE': ['Y','N'], 'FLAG_VISA': [1.0], 'Income_per_person': [360.0,400.0,520.0,800.0,1020.0], 'MARITAL_STATUS': [2.0,3.0,5.0], 'MONTHS_IN_RESIDENCE': [4.0,16.0 ], 'OCCUPATION_TYPE': [1.0,2.0,4.0,5.0], 'OTHER_INCOMES': [150.0], 'OTHER_INCOMES_dummy': [1.0], 'PAYMENT_DAY': [15.0,20.0,25.0], 'PERSONAL_MONTHLY_INCOME': [300.0,380.0,400.0,520.0,1540.0], 'PRODUCT': [2.0], 'PROFESSIONAL_PHONE_AREA_CODE_new': [19.0], 'PROFESSIONAL_ZIP_3': [400.0,620.0,660.0,840.0,950.0], 'PROFESSION_CODE': [2.0,9.0,11.0], 'QUANT_BANKING_ACCOUNTS': [1.0], 'QUANT_CARS': [1.0], 'QUANT_DEPENDANTS': [1.0,2.0,3.0], 'QUANT_SPECIAL_BANKING_ACCOUNTS': [1.0], 'RESIDENCE_TYPE': [2.0], 'RESIDENCIAL_PHONE_AREA_CODE_new': [0.0,41.0,63.0,84.0,91.0], 'RESIDENCIAL_STATE': ['SC%,%RO%,%RS','PR%,%MA%,%PB','AP%,%MG%,%SP%,%MS%,%PA%,%MT%,%PI','GO%,%RJ%,%CE','RR%,%PE%,%ES%,%RN%,%BA%,%AC%,%TO','AL%,%AM%,%DF%,%SE'], 'RESIDENCIAL_ZIP_3': [400.0,620.0,660.0,840.0,950.0], 'Total_Income': [1.0,2.0,4.0], 'Total_cards': [1.0], 'Unnamed: 0': [4500.0,10000.0,29500.0,35500.0,42000.0], 'dummy_profession': [1.0]}>'



IV = sc.iv(train_woe, 'default')
IV

th = 0.01
mylist = ['default']
for ii, i in enumerate(IV['variable']):
  if IV.iloc[ii]['info_value'] > th:
    mylist.append(i)

print(mylist)

!ls drive/MyDrive

train_woe_reduced = train_woe.iloc[:, train_woe.columns.isin(mylist)]
test_woe_reduced = test_woe.iloc[:, test_woe.columns.isin(mylist)]
df_woe_reduced = df_woe.iloc[:, df_woe.columns.isin(mylist)]

mylist1= []
for i in mylist:
  mylist1.append(i.split('_woe')[0])

# print(mylist1)

train_reduced = train.iloc[:, train.columns.isin(mylist1)]
test_reduced = test.iloc[:, test.columns.isin(mylist1)]
df_reduced = df.iloc[:, df.columns.isin(mylist1)]

train_woe_reduced.to_csv("drive/MyDrive/train_woe_reduced.csv", index = False)
test_woe_reduced.to_csv("drive/MyDrive/test_woe_reduced.csv", index = False)
df_woe_reduced.to_csv("drive/MyDrive/df_woe_reduced.csv", index = False)

train_woe.to_csv("drive/MyDrive/train_woe.csv", index = False)
test_woe.to_csv("drive/MyDrive/test_woe.csv", index = False)
df_woe.to_csv("drive/MyDrive/df_woe.csv", index = False)

train_reduced.to_csv("drive/MyDrive/train_reduced.csv", index = False)
test_reduced.to_csv("drive/MyDrive/test_reduced.csv", index = False)
df_reduced.to_csv("drive/MyDrive/df_reduced.csv", index = False)







# Commented out IPython magic to ensure Python compatibility.
from string import ascii_letters
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
# %matplotlib inline

corr = train_woe_reduced.corr()
corr = np.abs(corr)
mask = np.triu(np.ones_like(corr, dtype=bool))
f, ax = plt.subplots(figsize=(11, 9))
cmap = sns.diverging_palette(230, 20, as_cmap=True)
sns.heatmap(corr, mask=mask, cmap=cmap, vmax=1, center=0,
            square=True, linewidths=.5, cbar_kws={"shrink": .5})

import pandas as pd
train_woe_reduced = pd.read_csv('drive/MyDrive/train_woe_reduced.csv')
test_woe_reduced = pd.read_csv('drive/MyDrive/test_woe_reduced.csv')
df_woe_reduced = pd.read_csv('drive/MyDrive/df_woe_reduced.csv')

# train_woe_reduced['default_new'] = 0
# train_woe_reduced.loc[train_woe_reduced['default']==1, 'default_new'] = 0
# train_woe_reduced.loc[train_woe_reduced['default']==0, 'default_new'] = 1

# train_woe['default_new'] = 0
# train_woe.loc[train_woe['default']==1, 'default_new'] = 0
# train_woe.loc[train_woe['default']==0, 'default_new'] = 1

# train_reduced['default_new'] = 0
# train_reduced.loc[train_reduced['default']==1, 'default_new'] = 0
# train_reduced.loc[train_reduced['default']==0, 'default_new'] = 1

# test_woe_reduced['default_new'] = 0
# test_woe_reduced.loc[test_woe_reduced['default']==1, 'default_new'] = 0
# test_woe_reduced.loc[test_woe_reduced['default']==0, 'default_new'] = 1

# test_woe['default_new'] = 0
# test_woe.loc[test_woe['default']==1, 'default_new'] = 0
# test_woe.loc[test_woe['default']==0, 'default_new'] = 1

# test_reduced['default_new'] = 0
# test_reduced.loc[test_reduced['default']==1, 'default_new'] = 0
# test_reduced.loc[test_reduced['default']==0, 'default_new'] = 1

# df_woe_reduced['default_new'] = 0
# df_woe_reduced.loc[df_woe_reduced['default']==1, 'default_new'] = 0
# df_woe_reduced.loc[df_woe_reduced['default']==0, 'default_new'] = 1

# df_woe['default_new'] = 0
# df_woe.loc[df_woe['default']==1, 'default_new'] = 0
# df_woe.loc[df_woe['default']==0, 'default_new'] = 1

# df_reduced['default_new'] = 0
# df_reduced.loc[df_reduced['default']==1, 'default_new'] = 0
# df_reduced.loc[df_reduced['default']==0, 'default_new'] = 1

train_woe_reduced.drop(['PROFESSIONAL_ZIP_3_woe'], axis=1, inplace=True)
test_woe_reduced.drop(['PROFESSIONAL_ZIP_3_woe'], axis = 1, inplace=True)
df_woe_reduced.drop(['PROFESSIONAL_ZIP_3_woe'], axis = 1, inplace=True)
# train_woe_reduced.columns

# train_woe_reduced.drop(['default'], axis=1, inplace=True)
# test_woe_reduced.drop(['default'], axis = 1, inplace=True)
# df_woe_reduced.drop(['default'], axis = 1, inplace=True)

# train, test = sc.split_df(df_woe, y = 'default', ratio=0.7).values()

from sklearn.linear_model import LogisticRegressionCV, LogisticRegression
from sklearn.model_selection import GridSearchCV
import numpy as np

l_r = LogisticRegressionCV(Cs=np.arange(0.1, 1, 0.1), cv=10, penalty='elasticnet', solver='saga', tol=1e-4, 
                           max_iter=10000, class_weight='balanced', n_jobs=2, refit=True, l1_ratios = np.arange(0.1, 1.0, 0.01),
                           random_state=250542531, verbose=2)

# parameters = {'C': [10], 'l1_ratio': np.arange(0.01, 0.99, 0.1)}

# clf = LogisticRegression(penalty='elasticnet', solver='saga', tol=1e-4, max_iter=100, class_weight='balanced', n_jobs=2, random_state=250542531)
# gs = GridSearchCV(clf, parameters, refit=True)

l_r.fit(X = train_woe_reduced.iloc[:, ~train_woe_reduced.columns.isin(['default'])], y= train_woe_reduced['default'])

li = train_woe_reduced.columns[~train_woe_reduced.columns.isin(['default'])]
pd.concat([pd.DataFrame(li, columns=['variable']), pd.DataFrame(np.transpose(l_r.coef_), columns=['Logistic_Regression_Coef'])], axis=1)

l_r.intercept_

from sklearn.metrics import confusion_matrix, accuracy_score, f1_score
train_pred = l_r.predict(train_woe_reduced.iloc[:, ~train_woe_reduced.columns.isin(['default'])])
train_pred_prob = l_r.predict_proba(train_woe_reduced.iloc[:, ~train_woe_reduced.columns.isin(['default'])])

confusion_matrix(y_true = train_woe_reduced['default'], y_pred = train_pred)

train_accuracy = accuracy_score(y_true = train_woe_reduced['default'], y_pred = train_pred)
print(train_accuracy)

train_f1_score = f1_score(y_true = train_woe_reduced['default'], y_pred = train_pred)
print(train_f1_score)

test_pred = l_r.predict(test_woe_reduced.iloc[:, ~test_woe_reduced.columns.isin(['default'])])
test_pred_prob = l_r.predict_proba(test_woe_reduced.iloc[:, ~test_woe_reduced.columns.isin(['default'])])

confusion_matrix(y_true = test_woe_reduced['default'], y_pred = test_pred)

test_accuracy = accuracy_score(y_true = test_woe_reduced['default'], y_pred = test_pred)
print(test_accuracy)

test_f1_score = f1_score(y_true = test_woe_reduced['default'], y_pred = test_pred)
print(test_f1_score)

train_pred_prob_1 = [i[1] for i in train_pred_prob]
test_pred_prob_1 = [i[1] for i in test_pred_prob]

from sklearn.metrics import roc_curve, roc_auc_score
import matplotlib.pyplot as plt
lr_fp, lr_tp, lr_th = roc_curve(test_woe_reduced['default'], test_pred_prob_1)

auc = np.round(roc_auc_score(y_true = test_woe_reduced['default'], 
                             y_score = test_pred_prob_1),
               decimals = 3)

plt.plot(lr_fp,lr_tp,label="Scorecard, auc="+str(auc))
plt.legend(loc=4)
plt.show()

li = train_woe_reduced.columns[~train_woe_reduced.columns.isin(['default'])]

bins_adj_reduced = dict()

for i in li:
  i = i.split('_woe')[0]
  # A = bins_adj[i]['breaks'].values
  bins_adj_reduced[i] = bins_adj[i]
  # print(i)
  # print(bins_adj[i])

print(bins_adj_reduced.keys())

df_sc = sc.scorecard(bins_adj_reduced,         # bins from the WoE
                    l_r,  # Trained logistic regression
                    train_woe_reduced.columns[~train_woe_reduced.columns.isin(['default'])], # The column names in the trained LR
                    points0=800, # Base points
                    odds0=0.02, # Base odds bads:goods
                    pdo=50
                    ) # PDO 

df_sc

train_reduced_1 = pd.read_csv('drive/MyDrive/train_reduced.csv')
test_reduced_1 = pd.read_csv('drive/MyDrive/test_reduced.csv')

train_score = sc.scorecard_ply(train_reduced_1, df_sc, 
                               print_step=0)
test_score = sc.scorecard_ply(test_reduced_1, df_sc, 
                               print_step=0)

train_score['score'].max()

!pip install pwlf

import pwlf

# Define the curve with the ROC curve
piecewise_AUC = pwlf.PiecewiseLinFit(lr_fp, lr_tp)
res = piecewise_AUC.fit(10)

res = np. array([0, 0.12986065, 0.22288555, 0.25106273, 0.45337654, 0.58795426, 0.5985409 , 0.66864349, 0.73835212, 0.84700145, 1.])

ROC_CURVE = pd.DataFrame({'fp': lr_fp, 'threshold': lr_th})
ROC_CURVE

# Apply cuts!
cuts = piecewise_AUC.fit_with_breaks(res)

# predict for the determined points
xHat = np.linspace(min(lr_fp), max(lr_tp), num=10000)
yHat = piecewise_AUC.predict(xHat)

# plot the results
plt.figure()
plt.plot(lr_fp, lr_tp, 'o')
plt.plot(xHat, yHat, '-')
plt.show()

# Find probability associated with every cut
pbb_cuts = np.zeros_like(res)
i = 0

for j in res:
  temp = ROC_CURVE.loc[np.round(ROC_CURVE.fp, 2) == np.round(j, 2), 'threshold']
  pbb_cuts[i] = np.mean(temp)
  i += 1

pbb_cuts = np.flip(pbb_cuts)
pbb_cuts

pbb_cuts = np.append(pbb_cuts, 1)
pbb_cuts = np.insert(pbb_cuts, 0, 0)
pbb_cuts

pd_cut = pd.cut(test_woe_reduced['probs'], pbb_cuts)